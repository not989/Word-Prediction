{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.optimizers import RMSprop\n",
    "import pickle\n",
    "import sys\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, top_n=3):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed_text, n_words):\n",
    "    in_text, result = seed_text, seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = array(encoded)\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        #This is necessary( customized part)\n",
    "        preds = model.predict(encoded, verbose=0)[0]\n",
    "        \n",
    "        \n",
    "        next_indices = sample(preds, 5)\n",
    "#         print(next_indices)\n",
    "        ar = []\n",
    "        for indd in next_indices:\n",
    "            # map predicted word index to word\n",
    "            yhat = indd\n",
    "            out_word = ''\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == yhat:\n",
    "                    out_word = word\n",
    "                    break\n",
    "            # append to input\n",
    "            #print(out_word)\n",
    "            ar.append(out_word)\n",
    "            #in_text, result = out_word, result + ' ' + out_word\n",
    "        return ar\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = \"\"\" Jack and Jill went up the hill\\n\n",
    "# \t\tTo fetch a pail of water\\n\n",
    "# \t\tJack fell down and broke his crown\\n\n",
    "# \t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "\n",
    "# data = \"\"\" আমার নাম আবুল \\n\n",
    "# \t\tআবুলের জন্ম হইসে অনেক আগে \\n\n",
    "# \t\tএখন কেমন আছিস ?\\n\n",
    "# \t\tআবুল এখন কি করবো?\\n \"\"\"\n",
    "\n",
    "data = open('test_corpus_bn.txt', 'r', encoding='utf-8').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "# # saving\n",
    "# with open('tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # loading\n",
    "# with open('tokenizer.pickle', 'rb') as handle:\n",
    "#     tokenizer = pickle.load(handle)\n",
    "\n",
    "filename = \"tokenizer.pkl\"  \n",
    "with open(filename, 'wb') as file:  \n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2668\n"
     ]
    }
   ],
   "source": [
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 8672\n"
     ]
    }
   ],
   "source": [
    "# create word -> word sequences\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,0],sequences[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 2000, input_length=1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 47s - loss: 7.6043 - acc: 0.0202\n",
      "Epoch 2/10\n",
      " - 45s - loss: 6.7640 - acc: 0.0412\n",
      "Epoch 3/10\n",
      " - 47s - loss: 5.9003 - acc: 0.1169\n",
      "Epoch 4/10\n",
      " - 51s - loss: 4.9342 - acc: 0.2074\n",
      "Epoch 5/10\n",
      " - 49s - loss: 4.1039 - acc: 0.2940\n",
      "Epoch 6/10\n",
      " - 50s - loss: 3.4673 - acc: 0.3491\n",
      "Epoch 7/10\n",
      " - 51s - loss: 3.0072 - acc: 0.3704\n",
      "Epoch 8/10\n",
      " - 55s - loss: 2.6923 - acc: 0.3848\n",
      "Epoch 9/10\n",
      " - 53s - loss: 2.4829 - acc: 0.3826\n",
      "Epoch 10/10\n",
      " - 53s - loss: 2.3393 - acc: 0.3790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b5b78a5668>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(X, y, epochs=10, verbose=2)\n",
    "\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# filepath = \"best_model_pkl_format.pkl\"\n",
    "# checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# history = model.fit(X, y, validation_split=0.60, epochs=15, batch_size=15, callbacks=[checkpointer])\n",
    "# print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = \"best_model_pkl_format.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['জানি', 'এই', 'বলছি', 'সবাই', 'কিছু']\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(\"best_model_pkl_format.pkl\", 'rb'))\n",
    "tokenizer_1 = pickle.load(open(\"tokenizer.pkl\", 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# evaluate\n",
    "res = generate_seq(loaded_model, tokenizer_1, 'আমরা', 1)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
